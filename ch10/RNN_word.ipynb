{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Repeat', 'is', 'the', 'best', 'medicine', 'for', 'memory']\n"
     ]
    }
   ],
   "source": [
    "setence = \"Repeat is the best medicine for memory\".split()\n",
    "print(setence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Repeat', 'best', 'for', 'is', 'medicine', 'memory', 'the']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(setence))) # sorted()를 사용하여 중복을 제거하고 정렬\n",
    "print(vocab) # 단어의 순서로 정렬됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Repeat': 1, 'best': 2, 'for': 3, 'is': 4, 'medicine': 5, 'memory': 6, 'the': 7}\n",
      "{'Repeat': 1, 'best': 2, 'for': 3, 'is': 4, 'medicine': 5, 'memory': 6, 'the': 7, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {tkn:i for i, tkn in enumerate(vocab, 1)} # word를 리스트로 만들어 인덱스를 1부터 부여\n",
    "print(word_to_index) # 각 단어에 대한 정수 인코딩 결과 출력\n",
    "word_to_index['<unk>'] = 0 # <unk>는 OOV(Out-Of-Vocabulary) 단어를 의미. 단어 집합에 없는 단어들은 <unk> 토큰으로 인코딩\n",
    "print(word_to_index) # <unk> 토큰을 고려한 단어 집합 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[1, 4, 7, 2, 5, 3, 6]\n"
     ]
    }
   ],
   "source": [
    "print(word_to_index['for']) # 단어를 key로 하고 인덱스를 value로 하는 딕셔너리이므로 단어 'for'의 인덱스 출력\n",
    "encode=[word_to_index[token] for token in setence]\n",
    "print(encode) # 문장을 정수 인코딩한 결과 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(sentence, word_to_index):\n",
    "    encode=[word_to_index[t] for t in sentence]\n",
    "    input_sequnce = encode[:-1] # 마지막 단어 제외하고 저장\n",
    "    label_sequnce = encode[1:] # 첫 단어 제외하고 저장\n",
    "    input_sequnce = torch.LongTensor(input_sequnce).unsqueeze(0) # 배치 차원 추가\n",
    "    label_sequnce = torch.LongTensor([label_sequnce]) # label은 one-hot encoding 안 함 -> CrossEntropyLoss 사용\n",
    "    print(input_sequnce)\n",
    "    print(label_sequnce)\n",
    "    return input_sequnce, label_sequnce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4, 7, 2, 5, 3]])\n",
      "tensor([[4, 7, 2, 5, 3, 6]])\n",
      "tensor([[1, 4, 7, 2, 5, 3]])\n",
      "tensor([[4, 7, 2, 5, 3, 6]])\n"
     ]
    }
   ],
   "source": [
    "X,Y = build_data(setence, word_to_index) # 입력 시퀀스와 레이블 시퀀스 출력\n",
    "print(X) # 입력 시퀀스 출력\n",
    "print(Y) # 레이블 시퀀스 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True): # def__init__ : 클래스의 초기화 함수 \n",
    "        super(Net, self).__init__() # nn.Module 클래스의 속성들을 가지고 초기화\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=input_size) # 임베딩 층 생성. 단어 집합의 크기는 vocab_size이고 임베딩 벡터의 차원은 input_size\n",
    "        self.rnn=nn.RNN(input_size, hidden_size, batch_first=batch_first) # RNN 층 생성. input_size는 임베딩 벡터의 크기이며, hidden_size는 은닉 상태의 크기\n",
    "        self.fc=nn.Linear(hidden_size, vocab_size) # 출력층 생성. 은닉 상태의 크기는 hidden_size이며, 출력층의 뉴런은 단어 집합의 크기인 vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        output=self.embedding_layer(x) # 입력된 정수 인코딩을 임베딩 벡터로 변환\n",
    "        #embedding layer: 단어를 임베딩 벡터로 만드는 역할. 크기변화(batch_size, seq_len, embedding_size)\n",
    "        output, hidden = self.rnn(output) # RNN 층의 입력이자 임베딩 벡터를 RNN 층에 입력\n",
    "        # RNN 층 : 은닉 상태의 크기를 hidden_size로 하여 RNN을 구현. 크기변화(batch_size, seq_len, hidden_size)\n",
    "        # output: (batch_size, seq_len, hidden_size)\n",
    "        # hidden: (1, batch_size, hidden_size) -> 마지막 시점의 은닉 상태 크기 \n",
    "\n",
    "        output = self.fc(output)\n",
    "        # 최종 출력층에서는 RNN 층의 출력을 받아서 단어 예측 -> 크기변화(batch_size, seq_len, vocab_size) => (batch_size * seq_len, vocab_size)\n",
    "\n",
    "        return output.view(-1, output.size(2)) # 3D 텐서를 2D 텐서로 변환하여 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index) # 단어 집합의 크기는 임베딩 층의 입력 차원\n",
    "input_size = vocab_size\n",
    "hidden_size = 20 # 은닉 상태의 크기는 20 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Net(vocab_size, input_size, hidden_size, batch_first=True) # 모델 생성\n",
    "loss_function=nn.CrossEntropyLoss() # 손실 함수 생성\n",
    "optimizer = optim.Adam(params=model.parameters()) # 옵티마이저 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'Repeat', 2: 'best', 3: 'for', 4: 'is', 5: 'medicine', 6: 'memory', 7: 'the', 0: '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "index_to_word = {v:k for k,v in word_to_index.items()} # 정수 인코딩 된 결과를 단어로 변환하기 위해 사용 \n",
    "print(index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x000001A603EAE0C0>\n"
     ]
    }
   ],
   "source": [
    "decode = lambda y: [index_to_word.get(x) for x in y] # y에 대해서 단어로 변환하는 함수 생성 \n",
    "print(decode)  # 람다 표현식을 사용하여 정수 시퀀스를 단어 시퀀스로 변환하는 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [3, 4, 3, 3, 1, 4] str: Repeat for is for for Repeat is\n",
      "10 [6, 5, 3, 3, 1, 3] str: Repeat memory medicine for for Repeat for\n",
      "20 [6, 5, 3, 3, 3, 2] str: Repeat memory medicine for for for best\n",
      "30 [2, 5, 2, 5, 3, 6] str: Repeat best medicine best medicine for memory\n",
      "40 [6, 5, 2, 5, 3, 6] str: Repeat memory medicine best medicine for memory\n",
      "50 [6, 5, 2, 5, 3, 6] str: Repeat memory medicine best medicine for memory\n",
      "60 [4, 5, 2, 5, 3, 6] str: Repeat is medicine best medicine for memory\n",
      "70 [4, 5, 2, 5, 3, 6] str: Repeat is medicine best medicine for memory\n",
      "80 [4, 7, 2, 5, 3, 6] str: Repeat is the best medicine for memory\n",
      "90 [4, 7, 2, 5, 3, 6] str: Repeat is the best medicine for memory\n",
      "100 [4, 7, 2, 5, 3, 6] str: Repeat is the best medicine for memory\n",
      "110 [4, 7, 2, 5, 3, 6] str: Repeat is the best medicine for memory\n",
      "120 [4, 7, 2, 5, 3, 6] str: Repeat is the best medicine for memory\n",
      "130 [4, 7, 2, 5, 3, 6] str: Repeat is the best medicine for memory\n",
      "140 [4, 7, 2, 5, 3, 6] str: Repeat is the best medicine for memory\n",
      "150 [4, 7, 2, 5, 3, 6] str: Repeat is the best medicine for memory\n",
      "160 [4, 7, 2, 5, 3, 6] str: Repeat is the best medicine for memory\n",
      "170 [4, 7, 2, 5, 3, 6] str: Repeat is the best medicine for memory\n",
      "180 [4, 7, 2, 5, 3, 6] str: Repeat is the best medicine for memory\n",
      "190 [4, 7, 2, 5, 3, 6] str: Repeat is the best medicine for memory\n"
     ]
    }
   ],
   "source": [
    "for step in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    output=model(X) # 입력한 문장으로부터 예측된 문장 출력\n",
    "    loss=loss_function(output, Y.view(-1)) # 손실 계산\n",
    "    loss.backward() # 기울기 계산 \n",
    "    optimizer.step() # 학습 진행 \n",
    "\n",
    "\n",
    "    pred = output.softmax(-1).argmax(-1).tolist() # 예측된 결과는 원-핫 인코딩 형식 -> softmax 함수를 통해 확률로 변환 후 가장 높은 확률의 인덱스를 예측 값으로 설정 \n",
    "\n",
    "    if step%10 == 0:\n",
    "        print(step, pred, 'str:', ' '.join(['Repeat']+decode(pred))) # 10회 반복마다 예측 결과 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
